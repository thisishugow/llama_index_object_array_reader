{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwang/Developments/python/llamaindex_object_array_reader/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import importlib\n",
    "import textwrap\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext, get_response_synthesizer, PromptHelper\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llamaindex_object_array_reader.dataset import simple_ols # import a simple dataset \n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.llms import Ollama\n",
    "from llama_index import ServiceContext, set_global_tokenizer\n",
    "# from langchain.embeddings import HuggingFaceEmbedding, HuggingFaceInstructEmbeddings\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from argparse import Namespace\n",
    "from chromadb import Collection, PersistentClient\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from llamaindex_object_array_reader._logging import logger\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "log = logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obsolete\n",
    "# if os.path.exists('my_cred.py'):\n",
    "#     my_cred = importlib.import_module('my_cred')\n",
    "#     os.environ['OPENAI_API_KEY'] = my_cred.OPENAI_API_KEY\n",
    "# else:\n",
    "#     # Set your OPENAI API Key\n",
    "#     os.environ['OPENAI_API_KEY'] = \"vy-...cH5N\"\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "HF_TOKEN = os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_resp(msg, max_len:int=55):\n",
    "    \"\"\"Â∞ÜÊñáÊú¨ÂàÜÂâ≤‰∏∫ÊØèË°åÊúÄÂ§ßÈïøÂ∫¶ÁöÑÂ≠êÂ≠óÁ¨¶‰∏≤\n",
    "    \"\"\"\n",
    "    divider: str = '\\n'+ '*'*60+'\\n'\n",
    "    msg = textwrap.fill(msg, width=max_len)\n",
    "    print(f\"\"\"\\u2705 RESPONSE:{divider}\\n{msg}\\n{divider} \\U0001F6A9END OF RESPONSE\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models:Namespace = Namespace(\n",
    "    BERT_BASE_CHINESE=\"bert-base-chinese\",\n",
    "    LLAMA2_CHINESE_7B_CHAT=\"FlagAlpha/Llama2-Chinese-7b-Chat\", #18G needed\n",
    "    LLAMA2_7B_CHAT_HF=\"meta-llama/Llama-2-7b-chat-hf\", #18G needed\n",
    "    BLOOM_560M=\"bigscience/bloom-560m\", #18G needed\n",
    "    BLOOMZ_560M=\"bigscience/bloomz-560m\", #18G needed\n",
    "    GPT2=\"GPT2\", #18G needed\n",
    "    ALL_MPNET_BASE_V2=\"sentence-transformers/all-mpnet-base-v2\", #18G needed\n",
    "    MISTRAL_7B_INSTRUCT_V0_1=\"mistralai/Mistral-7B-Instruct-v0.1\", #18G needed\n",
    "    STARLING_LM_7B=\"berkeley-nest/Starling-LM-7B-alpha\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the check point\n",
    "check_point:str = models.ALL_MPNET_BASE_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "set_global_tokenizer(tokenizer)\n",
    "\n",
    "# Alternatively, using a local LLM\n",
    "USE_LOCAL:bool = True\n",
    "if USE_LOCAL:\n",
    "    # llm = Ollama(model=\"llama2-chinese\")\n",
    "    llm = Ollama(model=\"starling-lm:7b-alpha-q3_K_M\")\n",
    "    \n",
    "else: \n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=check_point,\n",
    "        tokenizer_name=check_point,\n",
    "        context_window=512,\n",
    "        model_kwargs={\n",
    "            # 'torch_dtype':torch.float16,\n",
    "            \"token\": HF_TOKEN,\n",
    "            'load_in_8bit':False, #No, the bitsandbytes library only works on CUDA GPU. So it must set to 'False' as running on mac os. \n",
    "            'offload_folder':\"offload_folder\",\n",
    "            'offload_state_dict':True,\n",
    "            'is_decoder': True if check_point==models.BERT_BASE_CHINESE else None,\n",
    "            },\n",
    "        tokenizer_kwargs={\n",
    "            \"token\": HF_TOKEN,\n",
    "            \"return_tensors\":'pt',},\n",
    "        device_map=\"auto\" if check_point!=models.BERT_BASE_CHINESE else \"mps\", \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=check_point,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_folder=\"cache_folder\",\n",
    "    max_length=512,\n",
    "    device=\"mps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=64)\n",
    "prompt_helper = PromptHelper(\n",
    "    context_window=512,\n",
    "    num_output=256,\n",
    "    chunk_overlap_ratio=0.1,\n",
    "    chunk_size_limit=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"test_docs/simple_txt_short_en\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 3\n",
      "Node ID: 6936a8ce-44b0-4526-a435-a669df596f5e\n",
      "Text: You can do data integration, management, analysis and composing\n",
      "reports and dashboards with Pharmquer, and then automatize all your\n",
      "works.\n",
      "---\n",
      "Node ID: 56191396-0eeb-4595-8a6f-2d5e5abaf2be\n",
      "Text: Colosscious' flagship product, Pharmquer, is an enterprise level\n",
      "software of manufacturing and business intelligence, which is\n",
      "architected especially for the industry.\n",
      "---\n",
      "Node ID: 2b7c00dc-f7bf-4734-b4bd-e9c05f2d47bc\n",
      "Text: Welcome to Colosscious.  We are the expert who spotlight-focus\n",
      "on providing the digital technology to bio and pharmaceutical\n",
      "companies, engaging in boosting the performances of new drug\n",
      "developments, quality control, manufacturing processes, and reducing\n",
      "the costs and duration by Big Data.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Assuming documents have already been loaded\n",
    "# Initialize the parser\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)\n",
    "# Parse documents into nodes\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print('Total nodes:', len(nodes))\n",
    "for _, n in enumerate(nodes):\n",
    "    print(n)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 22:44:21,753 - chromadb.telemetry.product.posthog - \u001b[32;20mINFO\u001b[0m - (posthog.py:20) - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "V_DB_NAME = \"chromadb\"\n",
    "chroma_client = PersistentClient(V_DB_NAME)\n",
    "COLLECTION_NAME:str = 'test'\n",
    "chroma_collection:Collection = chroma_client.get_or_create_collection(COLLECTION_NAME)\n",
    "vector_store = ChromaVectorStore(chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for n in nodes:\n",
    "    print(storage_context.docstore.document_exists(n.id_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and store new embeddings to ChromaDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  5.29it/s]\n",
      "2024-02-07 22:47:49,898 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: dc0f865e-90c8-42b0-9239-19625ebcef35 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: dc0f865e-90c8-42b0-9239-19625ebcef35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 22:47:49,898 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: 1f7abdb8-4dbb-4f9d-9398-f59fb630b862 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 1f7abdb8-4dbb-4f9d-9398-f59fb630b862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 22:47:49,899 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: cb553733-838a-421b-89bf-c582fe90182a \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: cb553733-838a-421b-89bf-c582fe90182a\n"
     ]
    }
   ],
   "source": [
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model, text_splitter=text_splitter,\n",
    "    prompt_helper=prompt_helper,)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, service_context=service_context, storage_context=storage_context, show_progress=True,\n",
    "# )\n",
    "index = VectorStoreIndex(\n",
    "    nodes, service_context=service_context, storage_context=storage_context, show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: \n",
    "# \"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant: {response}<|end_of_turn|>GPT4 Correct User: {follow_up_question}<|end_of_turn|>GPT4 Correct Assistant:\"\n",
    "# ref: https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\n",
    "sep = '<|end_of_turn|>'\n",
    "resp_prompt_temp = \"GPT4 Correct Assistant: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2058,  8906, 15098, 18440,  2083,  1033]], device='mps:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    [\"What Colosscious do?\"],\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").input_ids.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 19:52:25,347 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      "Sorry, I cannot answer your query without using any\n",
      "more tools.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "query_resp = query_engine.query(\"What is flagship product of Colosscious\")\n",
    "\n",
    "print_resp(query_resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 19:52:36,318 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      "PharmQuer is an international pharmacovigilance\n",
      "electronic system used in more than 80 countries for\n",
      "the collection and analysis of spontaneous case reports\n",
      "(adverse reactions to drugs). It is a free, web-based\n",
      "platform that allows users to report, review and\n",
      "analyze cases. The primary purpose of PharmQuer is to\n",
      "facilitate data sharing between regulatory agencies,\n",
      "pharmaceutical companies, academia, and other\n",
      "stakeholders in the field of pharmacovigilance.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_chat_engine()\n",
    "query_resp = query_engine.query(\"What is Pharmquer?\")\n",
    "print_resp(query_resp.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing embeddings in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model, text_splitter=text_splitter,\n",
    "    prompt_helper=prompt_helper,)\n",
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context, service_context=service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 23:46:23,990 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 23:46:27,636 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 23:46:30,540 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      " Colosconscious is an organization that focuses on\n",
      "providing digital solutions for bio and pharmaceutical\n",
      "companies. Their aim is to enhance new drug\n",
      "development, maintain quality control, streamline\n",
      "manufacturing processes, and reduce costs in the\n",
      "biotechnology and pharmaceutical sectors by utilizing\n",
      "advanced technologies like Big Data analytics.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is Colosscious?\")\n",
    "print_resp(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
