{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwang/Developments/python/llamaindex_object_array_reader/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import importlib\n",
    "import textwrap\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, StorageContext, get_response_synthesizer, PromptHelper\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llamaindex_object_array_reader.dataset import simple_ols # import a simple dataset \n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.llms import Ollama\n",
    "from llama_index import ServiceContext, set_global_tokenizer\n",
    "# from langchain.embeddings import HuggingFaceEmbedding, HuggingFaceInstructEmbeddings\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from argparse import Namespace\n",
    "from chromadb import Collection, PersistentClient\n",
    "from dotenv import load_dotenv\n",
    "from llamaindex_object_array_reader import ObjectArrayReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from llamaindex_object_array_reader._logging import logger\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "log = logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obsolete\n",
    "# if os.path.exists('my_cred.py'):\n",
    "#     my_cred = importlib.import_module('my_cred')\n",
    "#     os.environ['OPENAI_API_KEY'] = my_cred.OPENAI_API_KEY\n",
    "# else:\n",
    "#     # Set your OPENAI API Key\n",
    "#     os.environ['OPENAI_API_KEY'] = \"vy-...cH5N\"\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "HF_TOKEN = os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_resp(msg, max_len:int=55):\n",
    "    \"\"\"Â∞ÜÊñáÊú¨ÂàÜÂâ≤‰∏∫ÊØèË°åÊúÄÂ§ßÈïøÂ∫¶ÁöÑÂ≠êÂ≠óÁ¨¶‰∏≤\n",
    "    \"\"\"\n",
    "    divider: str = '\\n'+ '*'*60+'\\n'\n",
    "    msg = textwrap.fill(msg, width=max_len)\n",
    "    print(f\"\"\"\\u2705 RESPONSE:{divider}\\n{msg}\\n{divider} \\U0001F6A9END OF RESPONSE\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models:Namespace = Namespace(\n",
    "    BERT_BASE_CHINESE=\"bert-base-chinese\",\n",
    "    LLAMA2_CHINESE_7B_CHAT=\"FlagAlpha/Llama2-Chinese-7b-Chat\", #18G needed\n",
    "    LLAMA2_7B_CHAT_HF=\"meta-llama/Llama-2-7b-chat-hf\", #18G needed\n",
    "    BLOOM_560M=\"bigscience/bloom-560m\", #18G needed\n",
    "    BLOOMZ_560M=\"bigscience/bloomz-560m\", #18G needed\n",
    "    GPT2=\"GPT2\", #18G needed\n",
    "    ALL_MPNET_BASE_V2=\"sentence-transformers/all-mpnet-base-v2\", #18G needed\n",
    "    MISTRAL_7B_INSTRUCT_V0_1=\"mistralai/Mistral-7B-Instruct-v0.1\", #18G needed\n",
    "    STARLING_LM_7B=\"berkeley-nest/Starling-LM-7B-alpha\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the check point\n",
    "check_point:str = models.ALL_MPNET_BASE_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
    "set_global_tokenizer(tokenizer)\n",
    "\n",
    "# Alternatively, using a local LLM\n",
    "USE_LOCAL:bool = True\n",
    "if USE_LOCAL:\n",
    "    # llm = Ollama(model=\"llama2-chinese\")\n",
    "    llm = Ollama(model=\"starling-lm:7b-alpha-q3_K_M\")\n",
    "    \n",
    "else: \n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=check_point,\n",
    "        tokenizer_name=check_point,\n",
    "        context_window=512,\n",
    "        model_kwargs={\n",
    "            # 'torch_dtype':torch.float16,\n",
    "            \"token\": HF_TOKEN,\n",
    "            'load_in_8bit':False, #No, the bitsandbytes library only works on CUDA GPU. So it must set to 'False' as running on mac os. \n",
    "            'offload_folder':\"offload_folder\",\n",
    "            'offload_state_dict':True,\n",
    "            'is_decoder': True if check_point==models.BERT_BASE_CHINESE else None,\n",
    "            },\n",
    "        tokenizer_kwargs={\n",
    "            \"token\": HF_TOKEN,\n",
    "            \"return_tensors\":'pt',},\n",
    "        device_map=\"auto\" if check_point!=models.BERT_BASE_CHINESE else \"mps\", \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=check_point,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_folder=\"cache_folder\",\n",
    "    max_length=512,\n",
    "    device=\"mps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=64)\n",
    "prompt_helper = PromptHelper(\n",
    "    context_window=512,\n",
    "    num_output=256,\n",
    "    chunk_overlap_ratio=0.1,\n",
    "    chunk_size_limit=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"test_docs/simple_txt_short_en\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 3\n",
      "Node ID: ec94fbc2-06cf-41aa-b156-9c8753d101d1\n",
      "Text: You can do data integration, management, analysis and composing\n",
      "reports and dashboards with Pharmquer, and then automatize all your\n",
      "works.\n",
      "---\n",
      "Node ID: 91c1aaef-6216-4242-b187-906db3939929\n",
      "Text: Colosscious' flagship product, Pharmquer, is an enterprise level\n",
      "software of manufacturing and business intelligence, which is\n",
      "architected especially for the industry.\n",
      "---\n",
      "Node ID: 1dc13ad7-d352-4219-a63c-f174adb3c933\n",
      "Text: Welcome to Colosscious.  We are the expert who spotlight-focus\n",
      "on providing the digital technology to bio and pharmaceutical\n",
      "companies, engaging in boosting the performances of new drug\n",
      "developments, quality control, manufacturing processes, and reducing\n",
      "the costs and duration by Big Data.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Assuming documents have already been loaded\n",
    "# Initialize the parser\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)\n",
    "# Parse documents into nodes\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print('Total nodes:', len(nodes))\n",
    "for _, n in enumerate(nodes):\n",
    "    print(n)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 15:18:35,622 - chromadb.telemetry.product.posthog - \u001b[32;20mINFO\u001b[0m - (posthog.py:20) - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "V_DB_NAME = \"chromadb\"\n",
    "chroma_client = PersistentClient(V_DB_NAME)\n",
    "COLLECTION_NAME:str = 'test'\n",
    "chroma_collection:Collection = chroma_client.get_or_create_collection(COLLECTION_NAME)\n",
    "vector_store = ChromaVectorStore(chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for n in nodes:\n",
    "    print(storage_context.docstore.document_exists(n.id_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and store new embeddings to ChromaDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  5.29it/s]\n",
      "2024-02-07 22:47:49,898 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: dc0f865e-90c8-42b0-9239-19625ebcef35 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: dc0f865e-90c8-42b0-9239-19625ebcef35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 22:47:49,898 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: 1f7abdb8-4dbb-4f9d-9398-f59fb630b862 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 1f7abdb8-4dbb-4f9d-9398-f59fb630b862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 22:47:49,899 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: cb553733-838a-421b-89bf-c582fe90182a \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: cb553733-838a-421b-89bf-c582fe90182a\n"
     ]
    }
   ],
   "source": [
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model, text_splitter=text_splitter,\n",
    "    prompt_helper=prompt_helper,)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, service_context=service_context, storage_context=storage_context, show_progress=True,\n",
    "# )\n",
    "index = VectorStoreIndex(\n",
    "    nodes, service_context=service_context, storage_context=storage_context, show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: \n",
    "# \"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant: {response}<|end_of_turn|>GPT4 Correct User: {follow_up_question}<|end_of_turn|>GPT4 Correct Assistant:\"\n",
    "# ref: https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\n",
    "sep = '<|end_of_turn|>'\n",
    "resp_prompt_temp = \"GPT4 Correct Assistant: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2058,  8906, 15098, 18440,  2083,  1033]], device='mps:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    [\"What Colosscious do?\"],\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ").input_ids.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 19:52:25,347 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      "Sorry, I cannot answer your query without using any\n",
      "more tools.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "query_resp = query_engine.query(\"What is flagship product of Colosscious\")\n",
    "\n",
    "print_resp(query_resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-07 19:52:36,318 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      "PharmQuer is an international pharmacovigilance\n",
      "electronic system used in more than 80 countries for\n",
      "the collection and analysis of spontaneous case reports\n",
      "(adverse reactions to drugs). It is a free, web-based\n",
      "platform that allows users to report, review and\n",
      "analyze cases. The primary purpose of PharmQuer is to\n",
      "facilitate data sharing between regulatory agencies,\n",
      "pharmaceutical companies, academia, and other\n",
      "stakeholders in the field of pharmacovigilance.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_chat_engine()\n",
    "query_resp = query_engine.query(\"What is Pharmquer?\")\n",
    "print_resp(query_resp.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load existing embeddings in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model, text_splitter=text_splitter,\n",
    "    prompt_helper=prompt_helper,)\n",
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context, service_context=service_context\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:07,575 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: dc0f865e-90c8-42b0-9239-19625ebcef35 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: dc0f865e-90c8-42b0-9239-19625ebcef35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:07,576 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: 1f7abdb8-4dbb-4f9d-9398-f59fb630b862 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 1f7abdb8-4dbb-4f9d-9398-f59fb630b862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:07,577 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: cb553733-838a-421b-89bf-c582fe90182a \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: cb553733-838a-421b-89bf-c582fe90182a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:19,513 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:23,079 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:28,070 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:32,393 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:35,861 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:41,581 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:46,546 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:52,140 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 13:58:57,621 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      " Colosscious, also known as \"Colosconscious,\" is a\n",
      "specialized entity dedicated to providing digital\n",
      "technology solutions for bio and pharmaceutical\n",
      "companies. Their primary objectives include enhancing\n",
      "new drug development effectiveness, upholding quality\n",
      "standards, optimizing manufacturing processes, and\n",
      "reducing expenses related to these domains. It's\n",
      "essential to acknowledge that the context provided may\n",
      "contain a typo (Colosconscious instead of Colosscious),\n",
      "which could result in confusion. While it's possible\n",
      "that the accurate name is \"Colosconscious,\" without\n",
      "additional clarification or detailed context, this\n",
      "assumption cannot be confirmed with certainty.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is Colosscious?\")\n",
    "print_resp(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use llama_index_object_array_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x1': 97.98219999874924,\n",
       "  'x2': 99.84941752810117,\n",
       "  'x3': 100.9727776594234,\n",
       "  'y': 360.87650920565545},\n",
       " {'x1': 101.00077953260389,\n",
       "  'x2': 99.87874921228179,\n",
       "  'x3': 99.35642250227457,\n",
       "  'y': 361.50488035486944}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview: demo data\n",
    "simple_ols[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ObjectArrayReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x1      50 non-null     float64\n",
      " 1   x2      50 non-null     float64\n",
      " 2   x3      50 non-null     float64\n",
      " 3   y       50 non-null     float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 1.7 KB\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.schema.base import Document\n",
    "object_arrays:list[Document] = loader.load_data(file=simple_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(simple_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='1f219b5d-0518-4048-a8fa-c08a8e0ec816', embedding=None, metadata={'columns': \"['x1', 'x2', 'x3', 'y']\", 'schema': 'None', 'shape': '(50, 4)'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='01044d00146a997fca8953cf8ba579cb4492a76d451ca3aa31645e0e2e9bcb89', text='97.98219999874924, 99.84941752810117, 100.9727776594234, 360.87650920565545\\n101.00077953260389, 99.87874921228179, 99.35642250227457, 361.50488035486944\\n98.5109626677227, 100.7485502397903, 99.46465098250788, 359.8117609861218\\n100.77335929310553, 100.03722922045552, 99.86657209922947, 362.2336960397953\\n100.97359840386007, 99.1724799721807, 100.16093297144785, 362.1391160315852\\n100.18799255929102, 100.55900119891184, 100.61532849440285, 363.29752977180965\\n100.9157547652626, 98.61649241995889, 99.06726035297895, 359.7975894964005\\n101.04615952660859, 102.00920930524853, 100.16419028246959, 364.8003752715575\\n99.46321248760913, 100.23898461781165, 100.4603474082993, 361.9810830871964\\n101.01997365057879, 100.70311893925478, 100.35193718659701, 363.88982333927027\\n100.13690655914681, 100.07882115404048, 98.55349011863521, 359.46137480234387\\n101.06426034086174, 100.39468013724496, 99.59524654141205, 362.4291116312047\\n98.37827347228104, 103.02974783668428, 100.1611399406794, 362.8735393636648\\n99.97507192990182, 98.90754655604644, 99.80434032022505, 360.25326008000667\\n100.39532804084602, 100.00783015782618, 98.84412853695146, 360.1443934124746\\n100.53538350964381, 101.45593826545792, 101.35656192129933, 365.686428125867\\n101.82462124282185, 100.60168747124192, 101.17271531836492, 365.98868463340847\\n100.20083842899689, 100.08207154841824, 99.58637885148526, 361.20837231254296\\n101.21055853157635, 100.14337012301043, 97.83946914592629, 359.5083658057684\\n99.18284850767726, 99.36415064348877, 99.44933313879001, 359.2461732873584\\n98.68076906156287, 99.88606328063753, 100.63302113481906, 361.1047253879728\\n100.69262845160499, 100.95667625950557, 100.42976056344008, 363.89692658755564\\n99.28259711401759, 99.70388277366841, 101.68434722375862, 363.2876025473195\\n100.06352462146366, 98.88111045420715, 98.17662400762073, 357.68289832611293\\n99.30093777662633, 99.66957998231912, 101.04827928253668, 362.24405534917537\\n101.80808125496162, 98.90879253371892, 100.43144827373477, 363.22961316755584\\n100.87937300919184, 100.21857314642527, 98.7095417586287, 360.6345955200315\\n101.26476906248293, 100.78711425646654, 100.29822225418964, 364.14048570001194\\n99.91296978569375, 99.81832037463671, 100.93755148875996, 362.85330879179776\\n98.33923043666618, 101.41609851401992, 100.9069226929846, 362.5750535271962\\n99.16537289151567, 101.53101959448773, 98.68726411895874, 359.9607243551743\\n99.03719843850321, 101.11878894468508, 100.96547923761082, 363.1452988224573\\n99.43406666585818, 100.91739344461386, 99.36206117770445, 360.78473992459374\\n99.68590044334066, 99.48627235799435, 99.56930464452118, 360.089118523317\\n97.32232213736515, 99.97594783847929, 100.69691843112757, 359.8391638384025\\n99.39074229940556, 99.93236851645756, 98.2362250018063, 358.01656116479893\\n100.77590520414071, 99.69003697728576, 98.90965598576909, 360.3683307126241\\n99.68679150345386, 99.02927646327905, 101.76626758973612, 363.239031940659\\n100.64548154386631, 99.20184240211881, 100.01344022902087, 361.5760580247661\\n98.99353686788966, 99.96912376997722, 101.61919061254812, 363.11424644961215\\n99.61923454090937, 99.106406018837, 100.84958819151115, 361.7497627252443\\n99.07093312420517, 100.95979226903094, 99.95112469407546, 361.39111973582743\\n100.07233850453227, 99.7129946461946, 101.44068386026443, 363.74407241841885\\n99.72656237514776, 98.72754826499948, 99.97952513372913, 360.1084405895639\\n99.52984537048624, 102.18311836300242, 98.52851110733191, 360.68518093107895\\n101.14589750687492, 101.45663623211762, 100.70204076103222, 365.2772452093873\\n100.69113884387745, 100.5140731983062, 98.73201138024642, 360.73859881401285\\n100.97373793175905, 99.21840952279409, 98.11881715092599, 358.8678844756544\\n103.14715709169663, 98.60042261009588, 100.7938715049434, 364.9675528117923\\n100.6433422264419, 100.72347518969544, 99.54248360001975, 362.1927963663157', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_arrays[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2225 > 512). Running this sequence through the model will result in indexing errors\n",
      "Parsing nodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 49.78it/s]\n",
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 10.85it/s]\n",
      "2024-02-08 15:20:20,129 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: dc0f865e-90c8-42b0-9239-19625ebcef35 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: dc0f865e-90c8-42b0-9239-19625ebcef35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 15:20:20,130 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: 1f7abdb8-4dbb-4f9d-9398-f59fb630b862 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 1f7abdb8-4dbb-4f9d-9398-f59fb630b862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 15:20:20,130 - chromadb.segment.impl.vector.local_persistent_hnsw - \u001b[33;20mWARNING\u001b[0m - (local_persistent_hnsw.py:271) - Add of existing embedding ID: cb553733-838a-421b-89bf-c582fe90182a \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: cb553733-838a-421b-89bf-c582fe90182a\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embedding_model,)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=object_arrays, service_context=service_context,  storage_context=storage_context, show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='a044bf0f-199b-4cab-822a-b61305ba9495', embedding=None, metadata={'columns': \"['x1', 'x2', 'x3', 'y']\", 'schema': 'None', 'shape': '(50, 4)'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='9f4c001f1f1ad7d1636d4457b18f1cbb05d3359492cf2267e2666fabf435f140', text='97.98219999874924, 99.84941752810117, 100.9727776594234, 360.87650920565545\\n101.00077953260389, 99.87874921228179, 99.35642250227457, 361.50488035486944\\n98.5109626677227, 100.7485502397903, 99.46465098250788, 359.8117609861218\\n100.77335929310553, 100.03722922045552, 99.86657209922947, 362.2336960397953\\n100.97359840386007, 99.1724799721807, 100.16093297144785, 362.1391160315852\\n100.18799255929102, 100.55900119891184, 100.61532849440285, 363.29752977180965\\n100.9157547652626, 98.61649241995889, 99.06726035297895, 359.7975894964005\\n101.04615952660859, 102.00920930524853, 100.16419028246959, 364.8003752715575\\n99.46321248760913, 100.23898461781165, 100.4603474082993, 361.9810830871964\\n101.01997365057879, 100.70311893925478, 100.35193718659701, 363.88982333927027\\n100.13690655914681, 100.07882115404048, 98.55349011863521, 359.46137480234387\\n101.06426034086174, 100.39468013724496, 99.59524654141205, 362.4291116312047\\n98.37827347228104, 103.02974783668428, 100.1611399406794, 362.8735393636648\\n99.97507192990182, 98.90754655604644, 99.80434032022505, 360.25326008000667\\n100.39532804084602, 100.00783015782618, 98.84412853695146, 360.1443934124746\\n100.53538350964381, 101.45593826545792, 101.35656192129933, 365.686428125867\\n101.82462124282185, 100.60168747124192, 101.17271531836492, 365.98868463340847\\n100.20083842899689, 100.08207154841824, 99.58637885148526, 361.20837231254296\\n101.21055853157635, 100.14337012301043, 97.83946914592629, 359.5083658057684\\n99.18284850767726, 99.36415064348877, 99.44933313879001, 359.2461732873584\\n98.68076906156287, 99.88606328063753, 100.63302113481906, 361.1047253879728\\n100.69262845160499, 100.95667625950557, 100.42976056344008, 363.89692658755564\\n99.28259711401759, 99.70388277366841, 101.68434722375862, 363.2876025473195\\n100.06352462146366, 98.88111045420715, 98.17662400762073, 357.68289832611293\\n99.30093777662633, 99.66957998231912, 101.04827928253668, 362.24405534917537\\n101.80808125496162, 98.90879253371892, 100.43144827373477, 363.22961316755584\\n100.87937300919184, 100.21857314642527, 98.7095417586287, 360.6345955200315\\n101.26476906248293, 100.78711425646654, 100.29822225418964, 364.14048570001194\\n99.91296978569375, 99.81832037463671, 100.93755148875996, 362.85330879179776\\n98.33923043666618, 101.41609851401992, 100.9069226929846, 362.5750535271962\\n99.16537289151567, 101.53101959448773, 98.68726411895874, 359.9607243551743\\n99.03719843850321, 101.11878894468508, 100.96547923761082, 363.1452988224573\\n99.43406666585818, 100.91739344461386, 99.36206117770445, 360.78473992459374\\n99.68590044334066, 99.48627235799435, 99.56930464452118, 360.089118523317\\n97.32232213736515, 99.97594783847929, 100.69691843112757, 359.8391638384025\\n99.39074229940556, 99.93236851645756, 98.2362250018063, 358.01656116479893\\n100.77590520414071, 99.69003697728576, 98.90965598576909, 360.3683307126241\\n99.68679150345386, 99.02927646327905, 101.76626758973612, 363.239031940659\\n100.64548154386631, 99.20184240211881, 100.01344022902087, 361.5760580247661\\n98.99353686788966, 99.96912376997722, 101.61919061254812, 363.11424644961215\\n99.61923454090937, 99.106406018837, 100.84958819151115, 361.7497627252443\\n99.07093312420517, 100.95979226903094, 99.95112469407546, 361.39111973582743\\n100.07233850453227, 99.7129946461946, 101.44068386026443, 363.74407241841885\\n99.72656237514776, 98.72754826499948, 99.97952513372913, 360.1084405895639\\n99.52984537048624, 102.18311836300242, 98.52851110733191, 360.68518093107895\\n101.14589750687492, 101.45663623211762, 100.70204076103222, 365.2772452093873\\n100.69113884387745, 100.5140731983062, 98.73201138024642, 360.73859881401285\\n100.97373793175905, 99.21840952279409, 98.11881715092599, 358.8678844756544\\n103.14715709169663, 98.60042261009588, 100.7938715049434, 364.9675528117923\\n100.6433422264419, 100.72347518969544, 99.54248360001975, 362.1927963663157', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 15:12:13,321 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      " The dataset contains a total of 50 rows, as indicated\n",
      "by the shape of (50, 4) for both sets of data provided.\n",
      "However, it's important to note that the context\n",
      "information doesn't explicitly state that there are two\n",
      "sets of data provided. But based on the alternating\n",
      "columns in each block of data, we can infer that there\n",
      "are indeed two separate sets of data with 50 rows each.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How many values with in the dataset?\")\n",
    "print_resp(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 15:13:24,979 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      " There are three columns' names that start with 'x',\n",
      "which are 'x1', 'x2', and 'x3'.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How many columns' name starts with 'x'?\")\n",
    "print_resp(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 15:21:03,508 - httpx - \u001b[32;20mINFO\u001b[0m - (_client.py:1027) - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\" \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "‚úÖ RESPONSE:\n",
      "************************************************************\n",
      "\n",
      " To find the average of column 'x1', we need to sum all\n",
      "the values in column 'x1' and then divide by the total\n",
      "number of rows (50). Here are all the values in column\n",
      "'x1':  99.07093312420517, 100.84958819151115,\n",
      "99.72656237514776, ... , 100.97373793175905  Adding all\n",
      "the values gives us:  99.07093312420517 +\n",
      "100.84958819151115 + ... + 100.97373793175905 = (sum of\n",
      "all x1 values)  Now, we need to divide the sum by the\n",
      "total number of rows, which is 50:  (sum of all x1\n",
      "values) / 50 = average of column 'x1'  Without\n",
      "calculating the exact sum, we can see that the average\n",
      "value lies between 98.73 (lowest value) and 100.97\n",
      "(highest value). However, without performing the actual\n",
      "calculation, we cannot provide an exact numerical\n",
      "answer for the average of column 'x1'.  Please note\n",
      "that providing the exact average would require further\n",
      "calculations that go beyond the scope of this AI model.\n",
      "\n",
      "************************************************************\n",
      " üö©END OF RESPONSE\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the average of column 'x1'?\")\n",
    "print_resp(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97.982200</td>\n",
       "      <td>99.849418</td>\n",
       "      <td>100.972778</td>\n",
       "      <td>360.876509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101.000780</td>\n",
       "      <td>99.878749</td>\n",
       "      <td>99.356423</td>\n",
       "      <td>361.504880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.510963</td>\n",
       "      <td>100.748550</td>\n",
       "      <td>99.464651</td>\n",
       "      <td>359.811761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.773359</td>\n",
       "      <td>100.037229</td>\n",
       "      <td>99.866572</td>\n",
       "      <td>362.233696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.973598</td>\n",
       "      <td>99.172480</td>\n",
       "      <td>100.160933</td>\n",
       "      <td>362.139116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1          x2          x3           y\n",
       "0   97.982200   99.849418  100.972778  360.876509\n",
       "1  101.000780   99.878749   99.356423  361.504880\n",
       "2   98.510963  100.748550   99.464651  359.811761\n",
       "3  100.773359  100.037229   99.866572  362.233696\n",
       "4  100.973598   99.172480  100.160933  362.139116"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     97.982200\n",
      "1    101.000780\n",
      "2     98.510963\n",
      "3    100.773359\n",
      "4    100.973598\n",
      "Name: x1, dtype: float64\n",
      "Mean=  100.07520939069373\n"
     ]
    }
   ],
   "source": [
    "print(df['x1'][:5])\n",
    "print('Mean= ', df['x1'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
